{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy\n",
    "\n",
    "# twitter credentials\n",
    "consumer_key = os.getenv('TWITTER_CUSTOMER_KEY')\n",
    "consumer_secret = os.getenv('TWITTER_CUSTOMER_SECRET')\n",
    "\n",
    "api_twitter = tweepy.API(tweepy.OAuthHandler(consumer_key, consumer_secret))\n",
    "\n",
    "\n",
    "def twitter_get_text(uid:str) -> list:\n",
    "    if uid == '-':\n",
    "        return []\n",
    "    return list(i._json['text'] for i in api_twitter.user_timeline(screen_name=uid, count=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlopen\n",
    "from json import loads as json_load\n",
    "\n",
    "fb_access_url = \"https://graph.facebook.com/v2.6/{company}/posts\"\\\n",
    "                \"?access_token={fb_key}|{fb_secret}\"\n",
    "fb_access_url = fb_access_url.format(fb_key = os.getenv('FB_KEY'),\n",
    "                                     fb_secret = os.getenv('FB_SECRET'),\n",
    "                                     company = '{company}')\n",
    "\n",
    "    \n",
    "def facebook_get_text(uid:str) -> list:\n",
    "    if uid == '-':\n",
    "        return []\n",
    "    fb_data = urlopen(fb_access_url.format(company=uid)).read()\n",
    "    fb_data = json_load(fb_data.decode())\n",
    "    olist = []\n",
    "    for i in fb_data['data']:\n",
    "        try:\n",
    "            olist.append(i['message'])\n",
    "        except KeyError:\n",
    "            olist.append('')\n",
    "    return olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from json import loads as json_load\n",
    "\n",
    "wiki_access_url = \"https://en.wikipedia.org/w/api.php?format=json\"\\\n",
    "                \"&action=query&prop=extracts&exintro=&explaintext=&titles={company}\"\n",
    "\n",
    "\n",
    "def wiki_get_text(uid:str) -> list:\n",
    "    if uid == '-':\n",
    "        return []\n",
    "    # TODO: wikipedia get all text for page\n",
    "    wiki_data = urlopen(wiki_access_url.format(company=uid)).read()\n",
    "    wiki_data = json_load(wiki_data.decode())\n",
    "    wiki_data = wiki_data['query']['pages']\n",
    "    wiki_data = wiki_data[next(iter(wiki_data))]\n",
    "    return [wiki_data['extract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_set = [twitter_get_text, facebook_get_text, wiki_get_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPWORDS = set([\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "                 \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \n",
    "                 \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \n",
    "                 \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \n",
    "                 \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \n",
    "                 \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \n",
    "                 \"become\", \"becomes\", \"becoming\", \"b    een\", \"before\", \"beforehand\", \n",
    "                 \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \n",
    "                 \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \n",
    "                 \"co\", \"computer\", \"c    on\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \n",
    "                 \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \n",
    "                 \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \n",
    "                 \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \n",
    "                 \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \n",
    "                 \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \n",
    "                 \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\",\n",
    "                 \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \n",
    "                 \"hers\", \"herse\", \"him\", \"himse\", \"his\", \"how\", \"however\", \"hundred\", \n",
    "                 \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \n",
    "                 \"its\", \"itse\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \n",
    "                 \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \n",
    "                 \"moreover\", \"most\",     \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myse\", \"name\", \n",
    "                 \"namely\", \"neither\", \"never\", \"neverth    eless\", \"next\", \"nine\", \"no\", \"nobody\", \n",
    "                 \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \n",
    "                 \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \n",
    "                 \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \n",
    "                 \"please\", \"put\"    , \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \n",
    "                 \"seems\", \"serious\", \"several\", \"sh    e\", \"should\", \"show\", \"side\", \"since\", \n",
    "                 \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"some    one\", \"something\", \n",
    "                 \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \n",
    "                 \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \n",
    "                 \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \n",
    "                 \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \n",
    "                 \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too    \", \"top\", \"toward\", \n",
    "                 \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\",     \n",
    "                 \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \n",
    "                 \"whence\", \"wheneve    r\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \n",
    "                 \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \n",
    "                 \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \n",
    "                 \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", 'w/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_to_foo(f, data:str) -> list:\n",
    "    return f(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_words(string:str) -> list:\n",
    "    string = re.sub(r\"(http|@|#)\\S+\", \"\", string)\n",
    "    return [i for i in (x.lower() for x in filter(lambda x: len(x) > 0, re.split('[ .,!?:â€¦\\\"\\'\\-\\+]+', string)))\n",
    "            if not i in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "def get_top_ten_sequences(words:list, numw:int) -> list:\n",
    "    d = dict()\n",
    "    for i in words:\n",
    "        if len(i) < numw:\n",
    "            continue\n",
    "        t = list(i)\n",
    "        for j in range(len(t) - numw):\n",
    "            try:\n",
    "                d[frozenset(t[j:j+numw])] += 1\n",
    "            except KeyError:\n",
    "                d[frozenset(t[j:j+numw])] = 1\n",
    "    return sorted(d.items(), key=operator.itemgetter(1))[:-11:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from functools import reduce\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "RD = dict()\n",
    "\n",
    "\n",
    "def merge_lists(a, b):\n",
    "    c = []\n",
    "    list.extend(c, a)\n",
    "    list.extend(c, b)\n",
    "    return c\n",
    "\n",
    "\n",
    "def fstophrase(fs:frozenset)->str:\n",
    "    return ' '.join(fs)\n",
    "\n",
    "\n",
    "with open('links.csv', 'r') as csvfile:\n",
    "    companiesd = dict()\n",
    "    awd = dict()\n",
    "    for company in csv.reader(csvfile):\n",
    "        ftid = zip(source_set, company[1:]) # map fuction to social network id\n",
    "        ctexts = reduce(merge_lists, map(map_to_foo, source_set, company[1:]))\n",
    "        with open('{company}.txt'.format(company=company[0]), 'w') as cout:\n",
    "            for i in ctexts:\n",
    "                print(i, file=cout)\n",
    "        cseq = list(map(get_words, ctexts))\n",
    "        for words in cseq:\n",
    "            for word in words:\n",
    "                try:\n",
    "                    awd[word] += 1\n",
    "                except KeyError:\n",
    "                    awd[word] = 1\n",
    "        crezs = map(get_top_ten_sequences, repeat(cseq), range(1, 4))\n",
    "        crezd = list(map(lambda t: dict(map(lambda x: (fstophrase(x[0]), x[1]), t)), crezs))\n",
    "        companiesd[company[0]] = crezd\n",
    "    companiesd['all words'] = sorted(awd.items(), key=operator.itemgetter(1))[:-11:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "with open('result.txt', 'w') as pretty_output:\n",
    "    pprint.pprint(companiesd, pretty_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
